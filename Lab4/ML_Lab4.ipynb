{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNvu7aNho2lV",
        "outputId": "bfccd13f-ad95-492a-a9ca-e918019ea348"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 56.],\n",
            "        [ 81.],\n",
            "        [119.],\n",
            "        [ 22.],\n",
            "        [103.]])\n",
            "tensor([[125.7215],\n",
            "        [167.0218],\n",
            "        [174.0564],\n",
            "        [136.9420],\n",
            "        [156.2937]], grad_fn=<AddBackward0>)\n",
            "tensor(3134.3914, grad_fn=<DivBackward0>)\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-60-c47b6e16b4ed>:37: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:480.)\n",
            "  print(weight.grad)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "inputs = np.array([[73, 67, 43],[91, 88, 64],[87, 134, 58],[102, 43, 37],[69, 96, 70]], \n",
        "                  dtype='float32')\n",
        "targets = np.array([[56],[81],[119],[22],[103]],\n",
        "                 dtype='float32')\n",
        "\n",
        "# Convert inputs and targets to tensors\n",
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)\n",
        "#print(inputs)\n",
        "print(targets)\n",
        "\n",
        "# Weights and biases\n",
        "weight = torch.rand(1,3,requires_grad=True)\n",
        "#print(weight)\n",
        "weight = torch.t(weight)\n",
        "#print(weight)\n",
        "\n",
        "# Define the model\n",
        "temp = torch.matmul(inputs,weight)\n",
        "b = torch.tensor([[2.],[2.],[2.],[2.],[2.]])\n",
        "b.require_grad = True\n",
        "# Define the model\n",
        "prediction = temp+b\n",
        "#print(temp)\n",
        "print(prediction)\n",
        "# MSE loss\n",
        "# Compute loss\n",
        "\n",
        "costs = torch.sub(prediction,targets)\n",
        "loss_val = torch.square(costs).sum()/10\n",
        "print(loss_val)\n",
        "gradient = loss_val.backward()\n",
        "#weight.grad\n",
        "print(weight.grad)\n",
        "#print(torch.tensor.auto_grad.backward())"
      ]
    }
  ]
}